# A Comparative Study of Machine Learning and Deep Learning Models for Grammar Autocorrection

## ğŸ“ Dataset Overview

**Dataset** used in this project is the **Grammar-Correct Dataset** obtained from [Kaggle](https://www.kaggle.com/datasets).  

It contains slightly more than **2,000 English sentence pairs**, each consisting of:
- An **ungrammatical sentence**
- Its **corrected version**

The dataset covers a wide range of common grammatical errors, such as:
- Spelling and punctuation mistakes
- Verb tense inconsistencies
- Article misuses
- Sentence structure faults .. etc

---

## ğŸ“Š Dataset Structure

| Feature | Type | Description |
|--------|------|-------------|
| Serial Number | Numerical | Entry index |
| Error Type | Categorical | Classification of the grammar error |
| Ungrammatical Statement | Text | Raw sentence with grammar errors |
| Standard English | Text | Corrected sentence |

---

## ğŸ™Œ Contributions

The full preprocessing pipeline, model training, analysis, and reporting for Dataset were conducted by:

- **[Mariam-Elbishbeashy](https://github.com/Mariam-Elbishbeashy)**
- **[LauraLucasZ](https://github.com/LauraLucasZ)**

This work is part of an **original, research project** on grammatical error correction.

---

## ğŸ› ï¸ Preprocessing Steps

1. **Cleaning Special Characters**
   - Lowercasing
   - Retained `.`, `,`, `?`, alphanumeric and whitespace  
   - Removed unnecessary symbols

3. **Negation Normalization**  
   - E.g., `can't` â†’ `cannot`, `won't` â†’ `will not`

4. **Tokenization & POS Tagging**  
   - `nltk.word_tokenize` and Averaged Perceptron POS tagger

5. **Feature Engineering**  
   - POS Tag Sequences  
   - POS Embeddings (Word2Vec, 50-dim)  
   - Token Embeddings (Word2Vec, 100-dim)  
   - Bigram & Trigram Embeddings (50-dim each)

---

## ğŸ“ˆ Visualizations

- Top 5 grammatical error types
- Sentence length distributions
- Top 20 unigrams, bigrams, trigrams in ungrammatical data

---

## ğŸ¤– Models Applied

### Machine Learning:
- Naive Bayes  
- SVM  
- KNN  
- Decision Tree  
- XGBoost  
- Random Forest  

### Deep Learning:
- DistilBERT  
- T5-small  
- FLAN-T5-base  
- BART
- LSTM
- DistilBART

---

## ğŸ“Š Results Summary

## ğŸ“Š Machine Learning Results

### 10-Fold Cross-Validation

| Classifier     | Features   | Accuracy (%) | Precision (%) | Recall (%) | F1 Score (%) |
|----------------|------------|--------------|----------------|-------------|--------------|
| GaussianNB     | POS        | 53.12 Â± 2.69 | 52.60 Â± 2.17   | 63.62 Â± 3.22| 57.57 Â± 2.37 |
|                | Unigram    | 51.31 Â± 2.45 | 51.65 Â± 3.00   | 43.76 Â± 4.37| 47.25 Â± 3.08 |
|                | Bigram     | 50.42 Â± 2.00 | 50.66 Â± 3.00   | 31.12 Â± 3.91| 38.46 Â± 3.58 |
|                | Trigram    | 50.89 Â± 2.02 | 51.09 Â± 2.62   | 41.03 Â± 3.86| 45.45 Â± 3.12 |
| SVM            | POS        | 53.47 Â± 2.86 | 53.03 Â± 2.52   | 61.54 Â± 2.90| 56.95 Â± 2.46 |
|                | Unigram    | 51.39 Â± 2.65 | 51.36 Â± 2.58   | 54.81 Â± 4.34| 52.95 Â± 2.80 |
|                | Bigram     | 50.10 Â± 2.13 | 40.12 Â± 20.17  | 51.73 Â± 44.53| 38.00 Â± 30.08 |
|                | Trigram    | 49.41 Â± 0.92 | 35.67 Â± 18.82  | 30.74 Â± 38.17| 25.99 Â± 25.98 |
| XGBoost        | POS        | 54.14 Â± 2.90 | 53.51 Â± 2.50   | 64.57 Â± 2.81| 58.48 Â± 2.09 |
|                | Unigram    | 51.61 Â± 1.50 | 51.90 Â± 1.72   | 44.45 Â± 6.69| 47.63 Â± 3.98 |
|                | Bigram     | 47.57 Â± 2.39 | 47.40 Â± 2.54   | 42.72 Â± 2.66| 44.88 Â± 2.13 |
|                | Trigram    | 50.89 Â± 2.62 | 51.53 Â± 4.33   | 31.57 Â± 5.98| 38.85 Â± 5.28 |
| KNN            | POS        | 48.71 Â± 1.80 | 48.70 Â± 1.90   | 46.63 Â± 2.74| 47.60 Â± 1.84 |
|                | Unigram    | 45.91 Â± 2.77 | 45.93 Â± 2.74   | 45.98 Â± 3.11| 45.94 Â± 2.77 |
|                | Bigram     | 39.57 Â± 1.83 | 39.22 Â± 1.92   | 38.01 Â± 2.53| 38.59 Â± 2.14 |
|                | Trigram    | 39.40 Â± 3.49 | 38.74 Â± 3.20   | 36.17 Â± 3.99| 37.33 Â± 3.30 |
| Random Forest  | POS        | 46.04 Â± 2.46 | 45.96 Â± 2.48   | 45.84 Â± 4.03| 45.88 Â± 3.19 |
|                | Unigram    | 41.97 Â± 3.13 | 42.13 Â± 3.11   | 42.86 Â± 3.67| 42.46 Â± 3.19 |
|                | Bigram     | 38.95 Â± 2.16 | 39.22 Â± 2.06   | 40.19 Â± 2.36| 39.69 Â± 2.13 |
|                | Trigram    | 43.24 Â± 2.20 | 43.15 Â± 2.18   | 42.57 Â± 3.18| 42.82 Â± 2.46 |
| Decision Tree  | POS        | 43.58 Â± 2.13 | 43.30 Â± 2.19   | 41.43 Â± 2.48| 42.33 Â± 2.20 |
|                | Unigram    | 43.58 Â± 2.22 | 43.23 Â± 2.38   | 40.93 Â± 2.79| 42.03 Â± 2.45 |
|                | Bigram     | 47.03 Â± 2.17 | 46.96 Â± 2.17   | 45.54 Â± 2.89| 46.21 Â± 2.27 |
|                | Trigram    | 46.93 Â± 2.03 | 46.74 Â± 2.21   | 45.19 Â± 4.14| 45.92 Â± 3.15 |


### Train/Test Split (90%â€“10%)

| Classifier     | Features   | Accuracy (%) | Precision (%) | Recall (%) | F1 Score (%) |
|----------------|------------|--------------|----------------|-------------|--------------|
| GaussianNB     | POS        | 55.45        | 55.60          | 55.45       | 55.13        |
|                | Unigram    | 50.25        | 50.26          | 50.25       | 49.68        |
|                | Bigram     | 50.99        | 51.13          | 50.99       | 49.47        |
|                | Trigram    | 48.02        | 47.98          | 48.02       | 47.77        |
| SVM            | POS        | 55.20        | 55.36          | 55.20       | 54.86        |
|                | Unigram    | 52.48        | 52.48          | 52.48       | 52.45        |
|                | Bigram     | 50.99        | 50.99          | 50.99       | 50.96        |
|                | Trigram    | 50.99        | 50.99          | 50.99       | 50.96        |
| XGBoost        | POS        | 50.74        | 50.75          | 50.74       | 50.63        |
|                | Unigram    | 46.29        | 46.25          | 46.29       | 46.17        |
|                | Bigram     | 43.07        | 43.04          | 43.07       | 43.02        |
|                | Trigram    | 43.56        | 43.56          | 43.56       | 43.55        |
| KNN            | POS        | 51.49        | 51.49          | 51.49       | 51.47        |
|                | Unigram    | 48.02        | 48.01          | 48.02       | 47.97        |
|                | Bigram     | 41.09        | 41.05          | 41.09       | 41.02        |
|                | Trigram    | 40.59        | 39.54          | 40.59       | 39.06        |
| Random Forest  | POS        | 52.23        | 52.26          | 52.23       | 52.04        |
|                | Unigram    | 44.80        | 44.80          | 44.80       | 44.79        |
|                | Bigram     | 37.38        | 37.35          | 37.38       | 37.35        |
|                | Trigram    | 41.83        | 41.68          | 41.83       | 41.57        |
| Decision Tree  | POS        | 44.55        | 44.46          | 44.55       | 44.32        |
|                | Unigram    | 46.04        | 45.97          | 46.04       | 45.82        |
|                | Bigram     | 44.80        | 44.80          | 44.80       | 44.79        |
|                | Trigram    | 42.57        | 42.57          | 42.57       | 42.57        |

---

## ğŸ“Š Deep Learning Results

### Evaluation on Full Dataset + 5-Fold Cross Validation

| Model     | BLEU (Full) | Exact Match (%) | BLEU (CV) | Accuracy (%) | Precision (%) | Recall (%) | F1 Score (%) |
|-----------|-------------|------------------|-----------|----------------|----------------|-------------|---------------|
| DistilBERT| -           | -                | -         | 58.67 Â± 1.91   | 58.26 Â± 2.34   | 63.17 Â± 7.01| 60.31 Â± 2.33 |
| T5        | 72.71       | 45.39            | 66.86 Â± 0.53| 34.59 Â± 1.98  | 34.59 Â± 1.98  | 34.59 Â± 1.98| 34.59 Â± 1.98 |
| FLAN-T5   | 83.32       | 59.12            | 74.64 Â± 1.58| 48.61 Â± 2.20  | 48.61 Â± 2.20  | 48.61 Â± 2.20| 48.61 Â± 2.20 |
| BART      | **94.04**   | **86.87**        | 77.89 Â± 1.04| 54.56 Â± 2.40  | 54.56 Â± 2.40  | 54.56 Â± 2.40| 54.56 Â± 2.40 |

---

## ğŸ§¾ Example Predictions

| Ungrammatical Sentence                       | Reference (Corrected)                      | BART Prediction                             | FLAN-T5 Prediction                          | Match (BART) | Match (FLAN-T5) |
|---------------------------------------------|---------------------------------------------|---------------------------------------------|----------------------------------------------|--------------|-----------------|
| they was playing soccer last night          | they were playing soccer last night         | they were playing soccer last night         | they were playing soccer last night          | âœ… True      | âœ… True          |
| she will goes to the party tonight          | she will go to the party tonight            | she will go to the party tonight            | she will go to the party tonight             | âœ… True      | âœ… True          |
| the kids plays video games after school     | the kids play video games after school      | the kids play video games after school      | the kids play video games after school       | âœ… True      | âœ… True          |
| the computer not working properly           | the computer is not working properly        | the computer is not working properly        | the computer is not working properly         | âœ… True      | âœ… True          |
| i has been to paris three times             | i have been to paris three times            | i have been to paris three times            | i have been to paris three times             | âœ… True      | âœ… True          |
| the cat catch the mouse yesterday           | the cat caught the mouse yesterday          | the cat caught the mouse yesterday          | the cat caught the mouse yesterday           | âœ… True      | âœ… True          |
| she buy a new dress for the party           | she buys a new dress for the party          | she buys a new dress for the party          | she buys a new dress for the party           | âœ… True      | âœ… True          |
| the teacher teach the students in the classroom | the teacher teaches the students in the classroom | the teacher teaches the students in the classroom | the teacher taught the students in the classroom | âœ… True    | âŒ False         |
| the dog was chase the squirrel              | the dog was chasing the squirrel            | the dog was chasing the squirrel                 | the dog chased the squirrel                  | âœ… True    | âŒ False         |
| the manager gives a speech at the meeting   | the manager gave a speech at the meeting    | the manager gives a speech at the meeting   | the manager gave a speech at the meeting     | âŒ False     | âŒ False          |

These examples demonstrate that transformer-based models like **BART** and **FLAN-T5** are capable of performing accurate grammatical corrections, closely matching the target sentences.

---

## âœ… Conclusion

This project demonstrates a comprehensive approach to grammatical error correction using both traditional machine learning and modern deep learning models.  
Key findings include:
- **ML models** achieved moderate performance with POS features showing the best consistency.
- **Transformer-based models**, especially **BART**, significantly outperformed others, achieving high BLEU and exact match scores.
- **FLAN-T5** & **FLAN-T5**  also showed strong generalization capabilities while being lightweight and efficient.

The study reinforces the effectiveness of pretrained transformer architectures for nuanced language correction tasks.

---

## ğŸš€ Future Work

To further enhance this project, the following directions are proposed:
- **Data Augmentation**: Increase dataset size with synthetic error generation to improve model robustness.
- **Error Type-Specific Models**: Train specialized models for distinct grammar error types (e.g., tense, articles).
- **Deployment**: Wrap the best model into a web-based grammar correction tool for public use.


